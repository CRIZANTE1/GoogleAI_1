{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2edc81e382cf"
   },
   "source": [
    "##### Copyright 2023 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "906e07f6e562"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeadDkMiISin"
   },
   "source": [
    "# Gemini API: Function calling with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEXQ3OwKIa-O"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://ai.google.dev/tutorials/function_calling_python_quickstart\"><img src=\"https://developers.generativeai.google/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on Generative AI</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/tutorials/function_calling_python_quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/tutorials/function_calling_python_quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df1767a3d1cc"
   },
   "source": [
    "You can provide Gemini models with descriptions of functions, the model may ask you to call a function and send back the result to help the model handle your query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFPBKLapSCkM"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFNV1e3ASJha"
   },
   "source": [
    "### Install the Python SDK\n",
    "\n",
    "The Python SDK for the Gemini API, is contained in the [`google-generativeai`](https://pypi.org/project/google-generativeai/) package. Install the dependency using pip:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9OEoeosRTv-5"
   },
   "outputs": [],
   "source": [
    "#!pip install -U google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCFF5VSTbcAR"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRC2HngneEeQ"
   },
   "source": [
    "Import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TS9l5igubpHO"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHYFrFPjSGNq"
   },
   "source": [
    "### Set up your API key\n",
    "\n",
    "Before you can use the Gemini API, you must first obtain an API key. If you don't already have one, create a key with one click in Google AI Studio.\n",
    "\n",
    "<a class=\"button button-primary\" href=\"https://makersuite.google.com/app/apikey\" target=\"_blank\" rel=\"noopener noreferrer\">Get an API key</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHhsUxDTdw0W"
   },
   "source": [
    "In Colab, add the key to the secrets manager under the \"ðŸ”‘\" in the left panel. Give it the name `API_KEY`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmSlTHXxb5pV"
   },
   "source": [
    "Once you have the API key, pass it to the SDK. You can do this in two ways:\n",
    "\n",
    "* Put the key in the `GOOGLE_API_KEY` environment variable (the SDK will automatically pick it up from there).\n",
    "* Pass the key to `genai.configure(api_key=...)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ab9ASynfcIZn"
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    # Used to securely store your API key\n",
    "    from google.colab import userdata\n",
    "    \n",
    "    # Or use `os.getenv('API_KEY')` to fetch an environment variable.\n",
    "    GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
    "except ImportError:\n",
    "    import os\n",
    "    GOOGLE_API_KEY = os.environ['GOOGLE_API_KEY']\n",
    "    \n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass a list of functions to the `tools` argument when creating a `genai.GenerativeModel`.\n",
    "\n",
    "> Important: The function arguments require type annotations for this to work. A limited selection of argument types are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-1.0-pro',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=<google.generativeai.types.content_types.FunctionLibrary object at 0x10a5e3890>,\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiply(a:float, b:float):\n",
    "    \"\"\"returns a * b.\"\"\"\n",
    "    return a*b\n",
    "\n",
    "model = genai.GenerativeModel(model_name='gemini-1.0-pro',\n",
    "                              tools=[multiply])\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recomended way to use function calling is through the chat interface. The main reason is that `FunctionCalls` fit nicely into chat's multi-turn structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = model.start_chat(enable_automatic_function_calling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With automatic function calling enabled `chat.send_message` automatically calls your function if the model asks it to.\n",
    "\n",
    "It appears to simply return a text response, containing the correct answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That is 2508 mittens in total.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('I have 57 cats, each owns 44 mittens, how many mittens is that in total?')\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2508"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "57*44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look in the `ChatSession.history` you can see the sequence of events:\n",
    "\n",
    "1. You sent the question.\n",
    "2. The model replied with a `glm.FunctionCall`.\n",
    "3. The `genai.ChatSession` executed the function locally and sent the model back a `glm.FunctionResponse`.\n",
    "4. The model used the function output in its answer.\n",
    "\n",
    "Note that the model can potentially respond with multiple function calls before returning a text response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user -> text: \"I have 57 cats, each owns 44 mittens, how many mittens is that in total?\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "model -> function_call {\n",
      "  name: \"multiply\"\n",
      "  args {\n",
      "    fields {\n",
      "      key: \"b\"\n",
      "      value {\n",
      "        number_value: 44\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"a\"\n",
      "      value {\n",
      "        number_value: 57\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "user -> function_response {\n",
      "  name: \"multiply\"\n",
      "  response {\n",
      "    fields {\n",
      "      key: \"result\"\n",
      "      value {\n",
      "        number_value: 2508\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "model -> text: \"That is 2508 mittens in total.\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for content in chat.history:\n",
    "    print(content.role, \"->\", content.parts[0])\n",
    "    print('-'*80+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need more control, you can do any of this manually using chat without automatic function enabled, or using `GenerativeModel.generate_content`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured data extraction\n",
    "\n",
    "While the Gemini API doesn't currently have a json mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(model_name='gemini-1.0-pro')\n",
    "\n",
    "story = model.generate_content(\"Tell me a story about a girl with magic backpack and her family\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFz04WEgOwWp"
   },
   "source": [
    "## Low level access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Js4Y4mO20txL"
   },
   "source": [
    "The `google.ai.generativelanguage` client library provides access to the low level types required for function calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S53E0EE8TBUF"
   },
   "outputs": [],
   "source": [
    "import google.ai.generativelanguage as glm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you peek inside the model's `tools` attribute, you can see how it describes the function(s) you passed it to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a:float, b:float):\n",
    "    \"\"\"returns a * b.\"\"\"\n",
    "    return a*b\n",
    "\n",
    "model = genai.GenerativeModel(model_name='gemini-1.0-pro',\n",
    "                             tools=[multiply])\n",
    "\n",
    "model._tools.to_proto()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFD4U7ym04F5"
   },
   "source": [
    "This is a list of `glm.Tool` objects. Each (1 in this case) contains a list of `glm.FunctionDeclarations`, which describes a function and its arguments.\n",
    "\n",
    "Using these classes could be necessary if the SDK fails to automatically create the `glm.FunctionDeclarations`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY6RmFQ76FVu"
   },
   "source": [
    "Here is a declaration for the same multiply function written using the `glm` classes.\n",
    "\n",
    "Note that these classes just describe the function for the API, they don't include an implementation it. So using this doesn't work with automatic function calling, but functions don't always need an implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCwHM4WbC4wb"
   },
   "outputs": [],
   "source": [
    "calculator = glm.Tool(\n",
    "    function_declarations=[\n",
    "      glm.FunctionDeclaration(\n",
    "        name='multiply',\n",
    "        description=\"Returns the product of two numbers.\",\n",
    "        parameters=glm.Schema(\n",
    "            type=glm.Type.OBJECT,\n",
    "            properties={\n",
    "                'a':glm.Schema(type=glm.Type.NUMBER),\n",
    "                'b':glm.Schema(type=glm.Type.NUMBER)\n",
    "            },\n",
    "            required=['a','b']\n",
    "        )\n",
    "      )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jS6ruiTp6VBf"
   },
   "source": [
    "Give the model the calculator and ask it to do some arithmetic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwhWG22cIIDU"
   },
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-pro', tools=[calculator])\n",
    "chat = model.start_chat()\n",
    "\n",
    "response = chat.send_message(\n",
    "    f\"What's 234551 X 325552 ?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "517ca06297bb"
   },
   "source": [
    "Like before the model returns a `glm.FunctionCall` invoking the calculator's `multiply` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhey4QA0DTJf"
   },
   "outputs": [],
   "source": [
    "response.candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07eecbaedd5e"
   },
   "source": [
    "Execute the function yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88758eebfd5c"
   },
   "outputs": [],
   "source": [
    "fc = response.candidates[0].content.parts[0].function_call\n",
    "assert fc.name == 'multiply'\n",
    "\n",
    "result = fc.args['a'] * fc.args['b']\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6ef0e9651cf"
   },
   "source": [
    "Send the result to the model, to continue the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3c67066411e"
   },
   "outputs": [],
   "source": [
    "response = chat.send_message(\n",
    "    glm.Content(\n",
    "    parts=[glm.Part(\n",
    "        function_response = glm.FunctionResponse(\n",
    "          name='multiply',\n",
    "          response={'result': result}\n",
    "        )\n",
    "    )]\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7c032834f41"
   },
   "source": [
    "## Summary\n",
    "\n",
    "Basic function calling is supported in the SDK. Remember that it is easier to manage using chat-mode, because of the natural back and forth structure. You're in charge of of actually calling the functions and sending results back to the model so it can produce a text-response. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "function_calling_python_quickstart.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
