{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize large documents using LangChain and Gemini\n",
        "\n",
        "[Gemini](https://ai.google.dev/models/gemini) is a family of generative AI models that lets developers generate content and solve problems. These models are designed and trained to handle both text and images as input.\n",
        "\n",
        "[LangChain](https://www.langchain.com/) is a framework designed to make integration of Large Language Models (LLM) like Gemini easier for applications.\n",
        "\n",
        "In this notebook, you'll learn how to create an application to summarize large documents using Gemini and LangChain.\n"
      ],
      "metadata": {
        "id": "zsOBhAC7hjFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "First, you must install the packages and set the necessary environment variables.\n",
        "\n",
        "### Installation\n",
        "\n",
        "Install LangChain's Python library, `langchain`."
      ],
      "metadata": {
        "id": "iHj4T7hsx1EB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yERdO0eFJpb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f194932-60c2-4347-90de-830c1e7f692b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.1/803.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.7/205.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install LangChain's integration package for Gemini, `langchain-google-genai`."
      ],
      "metadata": {
        "id": "45MVc1stzNUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain-google-genai"
      ],
      "metadata": {
        "id": "FcMXJTN5JsfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grab an API Key\n",
        "\n"
      ],
      "metadata": {
        "id": "ycFMUTxn0VoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use Gemini you need an *API key*. You can create an API key with one click in [Google AI Studio](https://makersuite.google.com/).\n",
        "After creating the API key, you can either set an environment variable named `GOOGLE_API_KEY` to your API Key or pass the API key as an argument when creating the `ChatGoogleGenerativeAI` LLM using `LangChain`.\n",
        "\n",
        "In this tutorial, you will set the environment variable `GOOGLE_API_KEY` to configure Gemini to use your API key."
      ],
      "metadata": {
        "id": "e1dZNWvUzksX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell and paste the API key in the prompt\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = getpass.getpass('Gemini API Key:')"
      ],
      "metadata": {
        "id": "1RO5jvTTddtc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d8ef78-ebf6-4a67-b580-e6b9450e796b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize text\n",
        "\n",
        "In this tutorial, you are going to summarize the text from a website using the Gemini model integrated through LangChain.\n",
        "\n",
        "You'll perform the following steps to achieve the same:\n",
        "1. Read and parse the website data using LangChain.\n",
        "2. Chain together the following:\n",
        "    * A prompt for extracting the required input data from the parsed website data.\n",
        "    * A prompt for summarizing the text using LangChain.\n",
        "    * An LLM model (Gemini) for prompting.\n",
        "\n",
        "3. Run the created chain to prompt the model for the summary of the website data."
      ],
      "metadata": {
        "id": "i7wgsoiz418u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the required libraries"
      ],
      "metadata": {
        "id": "RhL92-zmSB6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.prompt_template import format_document"
      ],
      "metadata": {
        "id": "rAv0UicpKARZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read and parse the website data\n",
        "\n",
        "LangChain provides a wide variety of document loaders. To read the website data as a document, you will use the `WebBaseLoader` from LangChain.\n",
        "\n",
        "To know more about how to read and parse input data from different sources using the document loaders of LangChain, read LangChain's [document loaders guide](https://python.langchain.com/docs/integrations/document_loaders)."
      ],
      "metadata": {
        "id": "4tKpRvmMRX23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(\"https://blog.google/technology/ai/google-gemini-ai/#sundar-note\")\n",
        "docs = loader.load()\n",
        "\n",
        "print(docs)"
      ],
      "metadata": {
        "id": "TTgmyxXzKCSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Gemini\n",
        "\n",
        "You must import the `ChatGoogleGenerativeAI` LLM from LangChain to initialize your model.\n",
        " In this example you will use **gemini-pro**, as it supports text summarization. To know more about the text model, read Google AI's [language documentation](https://ai.google.dev/models/gemini).\n",
        "\n",
        "You can configure the model parameters such as ***temperature*** or ***top_p***,  by passing the appropriate values when creating the `ChatGoogleGenerativeAI` LLM.  To learn more about the parameters and their uses, read Google AI's [concepts guide](https://ai.google.dev/docs/concepts#model_parameters)."
      ],
      "metadata": {
        "id": "4xlf_F_4B6lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# If there is no env variable set for API key, you can pass the API key\n",
        "# to the parameter `google_api_key` of the `ChatGoogleGenerativeAI` function:\n",
        "# `google_api_key=\"key\"`.\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
        "                 temperature=0.7, top_p=0.85)"
      ],
      "metadata": {
        "id": "WWA9F0ZqB-8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create prompt templates\n",
        "\n",
        "You'll use LangChain's [PromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) to generate prompts for summarizing the text.\n",
        "\n",
        "To summarize the text from the website, you will need the following prompts.\n",
        "1. Prompt to extract the data from the output of `WebBaseLoader`, named `doc_prompt`\n",
        "2. Prompt for the LLM model (Gemini) to summarize the extracted text, named `llm_prompt`.\n",
        "\n",
        "In the `llm_prompt`, the variable `text` will be replaced later by the text from the website."
      ],
      "metadata": {
        "id": "6TECDzaUSTvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To extract data from WebBaseLoader\n",
        "doc_prompt = PromptTemplate.from_template(\"{page_content}\")\n",
        "\n",
        "# To query Gemini\n",
        "llm_prompt_template = \"\"\"Write a concise summary of the following:\n",
        "\"{text}\"\n",
        "CONCISE SUMMARY:\"\"\"\n",
        "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
        "\n",
        "print(llm_prompt)"
      ],
      "metadata": {
        "id": "rixvvvaNKLe_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93efd1bc-f8a1-4ff0-8c2a-458a008fd4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['text'] template='Write a concise summary of the following:\\n\"{text}\"\\nCONCISE SUMMARY:'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Stuff documents chain\n",
        "\n",
        "LangChain provides [Chains](https://python.langchain.com/docs/modules/chains/) for chaining together LLMs with each other or other components for complex applications. You will create a **Stuff documents chain** for this application. A **Stuff documents chain** lets you combine all the documents, insert them into the prompt and pass that prompt to the LLM.\n",
        "\n",
        "You can create a Stuff documents chain using the [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language).\n",
        "\n",
        "To learn more about different types of document chains, read LangChain's [chains guide](https://python.langchain.com/docs/modules/chains/document/)."
      ],
      "metadata": {
        "id": "-wPBMFyISh13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Stuff documents chain using LCEL.\n",
        "# This is called a chain because we are chaining\n",
        "# together different elements with the LLM.\n",
        "# In the following example, to create stuff chain,\n",
        "# you will combine content, prompt, LLM model and\n",
        "# output parser together like a chain using LCEL.\n",
        "#\n",
        "# The chain implements the following pipeline:\n",
        "# 1. Extract data from documents and save to variable `text`.\n",
        "# 2. This `text` is then passed to the prompt and input variable\n",
        "#    in prompt is populated.\n",
        "# 3. The prompt is then passed to the LLM (Gemini).\n",
        "# 4. Output from the LLM is passed through an output parser\n",
        "#    to structure the model response.\n",
        "\n",
        "stuff_chain = (\n",
        "    # Extract data from the documents and add to the key `text`.\n",
        "    {\n",
        "        \"text\": lambda docs: \"\\n\\n\".join(\n",
        "            format_document(doc, doc_prompt) for doc in docs\n",
        "        )\n",
        "    }\n",
        "    | llm_prompt         # Prompt for Gemini\n",
        "    | llm                # Gemini function\n",
        "    | StrOutputParser()  # output parser\n",
        ")"
      ],
      "metadata": {
        "id": "EMZomQdyKMr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt the model\n",
        "\n",
        "To generate the summary of the  the website data, pass the documents extracted using the `WebBaseLoader` (`docs`) to `invoke()`."
      ],
      "metadata": {
        "id": "5L0Tvk_5eQzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stuff_chain.invoke(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "k9_GxkA5ePRR",
        "outputId": "20cb52a4-b90c-4df8-86a6-0c9aea1c4729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Google introduces Gemini, its most capable AI model yet. Gemini is multimodal, flexible, and optimized for different sizes. It surpasses state-of-the-art performance on various benchmarks, including text, coding, and multimodal tasks. Gemini's capabilities include sophisticated reasoning, understanding text, images, audio, and advanced coding. It is designed with responsibility and safety at its core, undergoing comprehensive safety evaluations and incorporating safety classifiers. Gemini is being rolled out across Google products, including Bard, Pixel, Search, and Ads. Developers and enterprise customers can access Gemini Pro via the Gemini API. Gemini Ultra will be available to select partners and experts for early experimentation before a broader release. Gemini represents a new era of AI innovation, with future versions expected to advance planning, memory, and context processing capabilities.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "That's it. You have successfully created an LLM application to summarize text using LangChain and Gemini."
      ],
      "metadata": {
        "id": "nfrBsxUFgZzc"
      }
    }
  ]
}